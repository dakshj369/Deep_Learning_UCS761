{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "46eR-3sX0Ppy"
      },
      "outputs": [],
      "source": [
        "import numpy as np;\n",
        "import pandas as pd;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load Csv File"
      ],
      "metadata": {
        "id": "sov9qxZy1gwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('multiple_linear_regression_dataset.csv');"
      ],
      "metadata": {
        "id": "iTpWDfiO1gZU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(5))\n",
        "print(df.columns)\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCyQnJCQ1HGH",
        "outputId": "0f17aa7a-9602-418e-b542-2b8ac5f3b7f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age  experience  income\n",
            "0   25           1   30450\n",
            "1   30           3   35670\n",
            "2   47           2   31580\n",
            "3   32           5   40130\n",
            "4   43          10   47830\n",
            "Index(['age', 'experience', 'income'], dtype='object')\n",
            "(20, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input columns are  \n",
        "1. age\n",
        "2. experience\n",
        "\n",
        "The output column is **income**\n",
        "\n",
        "Our model needs to handle 2 features"
      ],
      "metadata": {
        "id": "x21cjAfa2dru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=df[[\"age\",\"experience\"]]\n",
        "y=df[\"income\"]\n"
      ],
      "metadata": {
        "id": "LFOTyajP2dXx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Shape of x is 20,2\n",
        "2. Shape of y is 20,1"
      ],
      "metadata": {
        "id": "QHCEmkeg3gq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbWyAR1d3gJx",
        "outputId": "8df78936-ebdb-4f39-d545-1c7550cc884b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 2)\n",
            "(20,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_features=X.shape[1]\n",
        "\n",
        "w=np.zeros(n_features)\n",
        "b=0.0"
      ],
      "metadata": {
        "id": "SBtJTfZ92UhG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. we need one weight because each feature will have a different weightage for calculation of the output\n",
        "\n",
        "2. bias is separate to shift the answer boundary not affect the importance of the features and also when the features values are zero is give default value of output\n",
        "\n",
        "3. if we initialise the values to very large values then the weights will not change much -> no learning will take place"
      ],
      "metadata": {
        "id": "T0OV3HCj5RRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X,w,b):\n",
        "  y_hat=X.dot(w)+b\n",
        "  return y_hat"
      ],
      "metadata": {
        "id": "Mj5Aykkt7oPz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. there is no activation function because we have to predict the regression value and we are already getting it through y-hat\n",
        "\n",
        "2. y_hat takes numeric values\n",
        "\n",
        "3. Logistic regression is used if we want to map the regression values to a classes\n",
        "such as marks->pass/fail"
      ],
      "metadata": {
        "id": "dvUp1BCV7705"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_square_error(y,y_hat):\n",
        "  loss=np.mean((y-y_hat)**2)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ZCM8lV4t77gy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. To make all errors positive and to give more penalty to large mistakes.\n",
        "\n",
        "2. if the on eprediction is very wrong then the squared error becomes very large, so it strongly influences the total loss and pushes the model to correct it.\n",
        "\n",
        "3. Absolute error penalizes all mistakes equally and is not smooth, which makes gradient-based learning harder and slower."
      ],
      "metadata": {
        "id": "q6dIk_DiHA64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradients(X,y,y_hat):\n",
        "  N=len(y)\n",
        "\n",
        "  dw=(2/N)*X.T.dot(y_hat-y)\n",
        "  db=(2/N)*np.sum(y_hat-y)\n",
        "\n",
        "  return dw,db"
      ],
      "metadata": {
        "id": "bQ32wCwj759Y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. y_hat=X*w+b\n",
        "\n",
        "    X appears in dw because we are taking the derivative of y_hat wrt w and hence X remains\n",
        "    but when we take derivative wrt to b the X term vanishes\n",
        "\n",
        "2. error term tells us that how wrong our prediction is. we are training using error\n",
        "\n",
        "    no error -> no updation\n",
        "    \n",
        "    large error -> large correction"
      ],
      "metadata": {
        "id": "XwVTnpPBCdyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(w,b,dw,db,lr):\n",
        "  w=w-lr*dw\n",
        "  b=b-lr*db\n",
        "  return w,b"
      ],
      "metadata": {
        "id": "m3h-rFPO96yE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.0001\n",
        "epochs=1000\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  y_hat=predict(X,w,b)\n",
        "  loss=mean_square_error(y,y_hat)\n",
        "  dw,db=compute_gradients(X,y,y_hat)\n",
        "  w,b=update_parameters(w,b,dw,db,lr)\n",
        "\n",
        "  if(epoch%100==0):\n",
        "    print(f\"Epoch {epoch}, Loss:{loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klE7_rmr-QIY",
        "outputId": "95e6b3f6-c23b-4fcf-bf3e-63c6cd7d92d9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss:1727049635.0\n",
            "Epoch 100, Loss:66491868.55311352\n",
            "Epoch 200, Loss:61752567.201190114\n",
            "Epoch 300, Loss:58616531.07847049\n",
            "Epoch 400, Loss:56528801.53951118\n",
            "Epoch 500, Loss:55126542.02946697\n",
            "Epoch 600, Loss:54172526.94885703\n",
            "Epoch 700, Loss:53511656.14292054\n",
            "Epoch 800, Loss:53042523.72795741\n",
            "Epoch 900, Loss:52698829.56325033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. loss decrease over epochs as the model learns better weights.\n",
        "\n",
        "2. That means the and initial vaues of w is not correct or the learning rate is very large\n",
        "\n",
        "3. assume learning rate as step size, and epochs as how long you walk.\n",
        "\n",
        "    Small steps → need many steps\n",
        "\n",
        "    Large steps → fewer steps but can miss the target"
      ],
      "metadata": {
        "id": "gCKAXDW0GBG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_candidate = np.array([4.5,68])\n",
        "predicted_salary = new_candidate.dot(w) + b\n",
        "print(predicted_salary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZXSa3Cx-0XZ",
        "outputId": "3f0aae2d-0446-4010-f931-3bbf8a2cbdb7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96993.4623777421\n"
          ]
        }
      ]
    }
  ]
}